{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJPNXmaaOICGga9cXG2b3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kukrejamanal/DAV_69/blob/main/69_EXP_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 7\n"
      ],
      "metadata": {
        "id": "DLpBKJAAF_ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Analyzation in Python**"
      ],
      "metadata": {
        "id": "9qt-i9STGRVs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7gehk9dPFd3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33280a35-1c77-476a-9a78-f80f5134185c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"[1] Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Text_mining'\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    first_paragraph = soup.find('p')\n",
        "\n",
        "    if first_paragraph:\n",
        "        print(first_paragraph.text)\n",
        "    else:\n",
        "        print('No <p> tags found on the website')\n",
        "else:\n",
        "    print('Failed to retrieve data from the website')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "A5kTt205GuJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f7226c6-a927-4278-97b7-ff21322a78e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = first_paragraph.text\n",
        "print(text)\n",
        "print(\"\\n Sentence tokenization: \\n\" , sent_tokenize(text))\n",
        "print(\"\\n Word tokenization: \\n\" , word_tokenize(text))"
      ],
      "metadata": {
        "id": "zDyOGBRBGzkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93bd4c15-9774-4a19-f10c-ddb8e4f47e0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"[1] Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\n",
            "\n",
            "\n",
            " Sentence tokenization: \n",
            " ['Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text.', 'It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.', '\"[1] Written resources may include websites, books, emails, reviews, and articles.', 'High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning.', 'According to Hotho et al.', '(2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.', '[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output.', \"'High quality' in text mining usually refers to some combination of relevance, novelty, and interest.\", 'Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).']\n",
            "\n",
            " Word tokenization: \n",
            " ['Text', 'mining', ',', 'text', 'data', 'mining', '(', 'TDM', ')', 'or', 'text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'high-quality', 'information', 'from', 'text', '.', 'It', 'involves', '``', 'the', 'discovery', 'by', 'computer', 'of', 'new', ',', 'previously', 'unknown', 'information', ',', 'by', 'automatically', 'extracting', 'information', 'from', 'different', 'written', 'resources', '.', '``', '[', '1', ']', 'Written', 'resources', 'may', 'include', 'websites', ',', 'books', ',', 'emails', ',', 'reviews', ',', 'and', 'articles', '.', 'High-quality', 'information', 'is', 'typically', 'obtained', 'by', 'devising', 'patterns', 'and', 'trends', 'by', 'means', 'such', 'as', 'statistical', 'pattern', 'learning', '.', 'According', 'to', 'Hotho', 'et', 'al', '.', '(', '2005', ')', 'we', 'can', 'distinguish', 'between', 'three', 'different', 'perspectives', 'of', 'text', 'mining', ':', 'information', 'extraction', ',', 'data', 'mining', ',', 'and', 'a', 'knowledge', 'discovery', 'in', 'databases', '(', 'KDD', ')', 'process', '.', '[', '2', ']', 'Text', 'mining', 'usually', 'involves', 'the', 'process', 'of', 'structuring', 'the', 'input', 'text', '(', 'usually', 'parsing', ',', 'along', 'with', 'the', 'addition', 'of', 'some', 'derived', 'linguistic', 'features', 'and', 'the', 'removal', 'of', 'others', ',', 'and', 'subsequent', 'insertion', 'into', 'a', 'database', ')', ',', 'deriving', 'patterns', 'within', 'the', 'structured', 'data', ',', 'and', 'finally', 'evaluation', 'and', 'interpretation', 'of', 'the', 'output', '.', \"'High\", 'quality', \"'\", 'in', 'text', 'mining', 'usually', 'refers', 'to', 'some', 'combination', 'of', 'relevance', ',', 'novelty', ',', 'and', 'interest', '.', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', ',', 'text', 'clustering', ',', 'concept/entity', 'extraction', ',', 'production', 'of', 'granular', 'taxonomies', ',', 'sentiment', 'analysis', ',', 'document', 'summarization', ',', 'and', 'entity', 'relation', 'modeling', '(', 'i.e.', ',', 'learning', 'relations', 'between', 'named', 'entities', ')', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "fdist = FreqDist(word_tokenize(text))\n",
        "print(fdist.most_common(2))\n",
        "\n",
        "print(\"Frequency of each word: \\n\")\n",
        "for word, freq in fdist.items():\n",
        "    print(f'{word}: {freq}')"
      ],
      "metadata": {
        "id": "9lZ1hdDFGuOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad3fc7f-6794-40c8-9f0e-19635693db2e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 22), ('text', 9)]\n",
            "Frequency of each word: \n",
            "\n",
            "Text: 2\n",
            "mining: 7\n",
            ",: 22\n",
            "text: 9\n",
            "data: 3\n",
            "(: 5\n",
            "TDM: 1\n",
            "): 5\n",
            "or: 1\n",
            "analytics: 1\n",
            "is: 2\n",
            "the: 8\n",
            "process: 3\n",
            "of: 9\n",
            "deriving: 2\n",
            "high-quality: 1\n",
            "information: 5\n",
            "from: 2\n",
            ".: 9\n",
            "It: 1\n",
            "involves: 2\n",
            "``: 2\n",
            "discovery: 2\n",
            "by: 4\n",
            "computer: 1\n",
            "new: 1\n",
            "previously: 1\n",
            "unknown: 1\n",
            "automatically: 1\n",
            "extracting: 1\n",
            "different: 2\n",
            "written: 1\n",
            "resources: 2\n",
            "[: 2\n",
            "1: 1\n",
            "]: 2\n",
            "Written: 1\n",
            "may: 1\n",
            "include: 2\n",
            "websites: 1\n",
            "books: 1\n",
            "emails: 1\n",
            "reviews: 1\n",
            "and: 9\n",
            "articles: 1\n",
            "High-quality: 1\n",
            "typically: 1\n",
            "obtained: 1\n",
            "devising: 1\n",
            "patterns: 2\n",
            "trends: 1\n",
            "means: 1\n",
            "such: 1\n",
            "as: 1\n",
            "statistical: 1\n",
            "pattern: 1\n",
            "learning: 2\n",
            "According: 1\n",
            "to: 2\n",
            "Hotho: 1\n",
            "et: 1\n",
            "al: 1\n",
            "2005: 1\n",
            "we: 1\n",
            "can: 1\n",
            "distinguish: 1\n",
            "between: 2\n",
            "three: 1\n",
            "perspectives: 1\n",
            ":: 1\n",
            "extraction: 2\n",
            "a: 2\n",
            "knowledge: 1\n",
            "in: 2\n",
            "databases: 1\n",
            "KDD: 1\n",
            "2: 1\n",
            "usually: 3\n",
            "structuring: 1\n",
            "input: 1\n",
            "parsing: 1\n",
            "along: 1\n",
            "with: 1\n",
            "addition: 1\n",
            "some: 2\n",
            "derived: 1\n",
            "linguistic: 1\n",
            "features: 1\n",
            "removal: 1\n",
            "others: 1\n",
            "subsequent: 1\n",
            "insertion: 1\n",
            "into: 1\n",
            "database: 1\n",
            "within: 1\n",
            "structured: 1\n",
            "finally: 1\n",
            "evaluation: 1\n",
            "interpretation: 1\n",
            "output: 1\n",
            "'High: 1\n",
            "quality: 1\n",
            "': 1\n",
            "refers: 1\n",
            "combination: 1\n",
            "relevance: 1\n",
            "novelty: 1\n",
            "interest: 1\n",
            "Typical: 1\n",
            "tasks: 1\n",
            "categorization: 1\n",
            "clustering: 1\n",
            "concept/entity: 1\n",
            "production: 1\n",
            "granular: 1\n",
            "taxonomies: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "document: 1\n",
            "summarization: 1\n",
            "entity: 1\n",
            "relation: 1\n",
            "modeling: 1\n",
            "i.e.: 1\n",
            "relations: 1\n",
            "named: 1\n",
            "entities: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "lOPD7rcXGuHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3612bf-cd25-4237-be1f-dbd8098203b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "M0lETPLWGuCP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b417f553-858c-421f-97d3-d0b6f4027f51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'be', 'if', 'me', 'ourselves', 'out', \"aren't\", 'too', 'had', \"shouldn't\", 'll', 'of', 'most', 'mightn', 'or', 'didn', 'up', \"needn't\", 'and', 'don', 'yours', \"didn't\", 'from', 'for', 're', 'm', 'by', 'he', 'were', 'yourself', 'hasn', 'ain', 'we', 'itself', \"she's\", 'hadn', 'aren', 'to', 'just', \"you'd\", \"wouldn't\", 'himself', 'both', 'how', \"hadn't\", 'at', 'over', 'same', 'a', 'any', 'it', 'my', \"wasn't\", 'own', 'couldn', 'wasn', 'which', 'there', \"couldn't\", \"doesn't\", 'above', \"you've\", 'our', 'has', 'on', 'so', 'under', 'in', \"mustn't\", 'his', 'down', 'an', 'between', \"you're\", 't', 'why', 'haven', \"don't\", 'have', 'theirs', 'i', 'was', 'what', 'about', 'shouldn', \"weren't\", 'did', 'very', 'nor', 've', 'this', 'such', 'again', 'hers', 'will', 'is', 'until', 'are', 'myself', 'during', 'while', \"won't\", 'whom', 'with', 'few', 'needn', 'below', 'these', 'been', \"mightn't\", 'yourselves', 'ma', 'here', 'being', 'd', 'ours', 'as', 'that', \"that'll\", 'more', 'now', 'not', 'some', 'won', 'her', 'than', 'themselves', 'shan', 'the', 'does', 'doesn', 'do', 'mustn', 'through', 'isn', 'wouldn', 'off', 'but', 'then', 'they', 'against', 'because', 'y', 'herself', 'weren', 'them', 'before', 'when', 's', \"should've\", \"it's\", 'can', 'once', 'further', 'those', \"shan't\", 'your', 'she', 'into', 'after', \"haven't\", 'who', 'am', 'their', 'all', 'you', 'no', \"hasn't\", 'o', 'each', 'him', 'only', \"isn't\", 'its', 'other', 'should', 'where', 'having', \"you'll\", 'doing'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "filtered_tokens=[]\n",
        "for w in word_tokenize(text):\n",
        "    if w not in stop_words:\n",
        "         filtered_tokens.append(w)\n",
        "\n",
        "print(\"Tokenized Words:\",word_tokenize(text))\n",
        "print(\"Filterd Tokens:\",filtered_tokens)"
      ],
      "metadata": {
        "id": "FQdVakNIGt_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd4554f-cc40-43da-d521-fd7e22bf6a55"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Words: ['Text', 'mining', ',', 'text', 'data', 'mining', '(', 'TDM', ')', 'or', 'text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'high-quality', 'information', 'from', 'text', '.', 'It', 'involves', '``', 'the', 'discovery', 'by', 'computer', 'of', 'new', ',', 'previously', 'unknown', 'information', ',', 'by', 'automatically', 'extracting', 'information', 'from', 'different', 'written', 'resources', '.', '``', '[', '1', ']', 'Written', 'resources', 'may', 'include', 'websites', ',', 'books', ',', 'emails', ',', 'reviews', ',', 'and', 'articles', '.', 'High-quality', 'information', 'is', 'typically', 'obtained', 'by', 'devising', 'patterns', 'and', 'trends', 'by', 'means', 'such', 'as', 'statistical', 'pattern', 'learning', '.', 'According', 'to', 'Hotho', 'et', 'al', '.', '(', '2005', ')', 'we', 'can', 'distinguish', 'between', 'three', 'different', 'perspectives', 'of', 'text', 'mining', ':', 'information', 'extraction', ',', 'data', 'mining', ',', 'and', 'a', 'knowledge', 'discovery', 'in', 'databases', '(', 'KDD', ')', 'process', '.', '[', '2', ']', 'Text', 'mining', 'usually', 'involves', 'the', 'process', 'of', 'structuring', 'the', 'input', 'text', '(', 'usually', 'parsing', ',', 'along', 'with', 'the', 'addition', 'of', 'some', 'derived', 'linguistic', 'features', 'and', 'the', 'removal', 'of', 'others', ',', 'and', 'subsequent', 'insertion', 'into', 'a', 'database', ')', ',', 'deriving', 'patterns', 'within', 'the', 'structured', 'data', ',', 'and', 'finally', 'evaluation', 'and', 'interpretation', 'of', 'the', 'output', '.', \"'High\", 'quality', \"'\", 'in', 'text', 'mining', 'usually', 'refers', 'to', 'some', 'combination', 'of', 'relevance', ',', 'novelty', ',', 'and', 'interest', '.', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', ',', 'text', 'clustering', ',', 'concept/entity', 'extraction', ',', 'production', 'of', 'granular', 'taxonomies', ',', 'sentiment', 'analysis', ',', 'document', 'summarization', ',', 'and', 'entity', 'relation', 'modeling', '(', 'i.e.', ',', 'learning', 'relations', 'between', 'named', 'entities', ')', '.']\n",
            "Filterd Tokens: ['Text', 'mining', ',', 'text', 'data', 'mining', '(', 'TDM', ')', 'text', 'analytics', 'process', 'deriving', 'high-quality', 'information', 'text', '.', 'It', 'involves', '``', 'discovery', 'computer', 'new', ',', 'previously', 'unknown', 'information', ',', 'automatically', 'extracting', 'information', 'different', 'written', 'resources', '.', '``', '[', '1', ']', 'Written', 'resources', 'may', 'include', 'websites', ',', 'books', ',', 'emails', ',', 'reviews', ',', 'articles', '.', 'High-quality', 'information', 'typically', 'obtained', 'devising', 'patterns', 'trends', 'means', 'statistical', 'pattern', 'learning', '.', 'According', 'Hotho', 'et', 'al', '.', '(', '2005', ')', 'distinguish', 'three', 'different', 'perspectives', 'text', 'mining', ':', 'information', 'extraction', ',', 'data', 'mining', ',', 'knowledge', 'discovery', 'databases', '(', 'KDD', ')', 'process', '.', '[', '2', ']', 'Text', 'mining', 'usually', 'involves', 'process', 'structuring', 'input', 'text', '(', 'usually', 'parsing', ',', 'along', 'addition', 'derived', 'linguistic', 'features', 'removal', 'others', ',', 'subsequent', 'insertion', 'database', ')', ',', 'deriving', 'patterns', 'within', 'structured', 'data', ',', 'finally', 'evaluation', 'interpretation', 'output', '.', \"'High\", 'quality', \"'\", 'text', 'mining', 'usually', 'refers', 'combination', 'relevance', ',', 'novelty', ',', 'interest', '.', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', ',', 'text', 'clustering', ',', 'concept/entity', 'extraction', ',', 'production', 'granular', 'taxonomies', ',', 'sentiment', 'analysis', ',', 'document', 'summarization', ',', 'entity', 'relation', 'modeling', '(', 'i.e.', ',', 'learning', 'relations', 'named', 'entities', ')', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "punctuations=list(string.punctuation)\n",
        "\n",
        "filtered_tokens2=[]\n",
        "\n",
        "for i in filtered_tokens:\n",
        "    if i not in punctuations:\n",
        "        filtered_tokens2.append(i)\n",
        "\n",
        "print(\"Filterd Tokens After Removing Punctuations:\",filtered_tokens2)"
      ],
      "metadata": {
        "id": "o83qzyagGtqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5605ab-0203-4f88-9f3a-dc0d68d06f25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filterd Tokens After Removing Punctuations: ['Text', 'mining', 'text', 'data', 'mining', 'TDM', 'text', 'analytics', 'process', 'deriving', 'high-quality', 'information', 'text', 'It', 'involves', '``', 'discovery', 'computer', 'new', 'previously', 'unknown', 'information', 'automatically', 'extracting', 'information', 'different', 'written', 'resources', '``', '1', 'Written', 'resources', 'may', 'include', 'websites', 'books', 'emails', 'reviews', 'articles', 'High-quality', 'information', 'typically', 'obtained', 'devising', 'patterns', 'trends', 'means', 'statistical', 'pattern', 'learning', 'According', 'Hotho', 'et', 'al', '2005', 'distinguish', 'three', 'different', 'perspectives', 'text', 'mining', 'information', 'extraction', 'data', 'mining', 'knowledge', 'discovery', 'databases', 'KDD', 'process', '2', 'Text', 'mining', 'usually', 'involves', 'process', 'structuring', 'input', 'text', 'usually', 'parsing', 'along', 'addition', 'derived', 'linguistic', 'features', 'removal', 'others', 'subsequent', 'insertion', 'database', 'deriving', 'patterns', 'within', 'structured', 'data', 'finally', 'evaluation', 'interpretation', 'output', \"'High\", 'quality', 'text', 'mining', 'usually', 'refers', 'combination', 'relevance', 'novelty', 'interest', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', 'text', 'clustering', 'concept/entity', 'extraction', 'production', 'granular', 'taxonomies', 'sentiment', 'analysis', 'document', 'summarization', 'entity', 'relation', 'modeling', 'i.e.', 'learning', 'relations', 'named', 'entities']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Oc5cGJ_LF-4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed_words=[]\n",
        "\n",
        "for w in filtered_tokens2:\n",
        "     stemmed_words.append(ps.stem(w))\n",
        "\n",
        "print(\"Filtered Tokens After Removing Punctuations:\",filtered_tokens2)\n",
        "print(\"Stemmed Tokens:\",stemmed_words)"
      ],
      "metadata": {
        "id": "1ZwC7LIVINmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da42737-500f-4864-e07e-b46ff330f75d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens After Removing Punctuations: ['Text', 'mining', 'text', 'data', 'mining', 'TDM', 'text', 'analytics', 'process', 'deriving', 'high-quality', 'information', 'text', 'It', 'involves', '``', 'discovery', 'computer', 'new', 'previously', 'unknown', 'information', 'automatically', 'extracting', 'information', 'different', 'written', 'resources', '``', '1', 'Written', 'resources', 'may', 'include', 'websites', 'books', 'emails', 'reviews', 'articles', 'High-quality', 'information', 'typically', 'obtained', 'devising', 'patterns', 'trends', 'means', 'statistical', 'pattern', 'learning', 'According', 'Hotho', 'et', 'al', '2005', 'distinguish', 'three', 'different', 'perspectives', 'text', 'mining', 'information', 'extraction', 'data', 'mining', 'knowledge', 'discovery', 'databases', 'KDD', 'process', '2', 'Text', 'mining', 'usually', 'involves', 'process', 'structuring', 'input', 'text', 'usually', 'parsing', 'along', 'addition', 'derived', 'linguistic', 'features', 'removal', 'others', 'subsequent', 'insertion', 'database', 'deriving', 'patterns', 'within', 'structured', 'data', 'finally', 'evaluation', 'interpretation', 'output', \"'High\", 'quality', 'text', 'mining', 'usually', 'refers', 'combination', 'relevance', 'novelty', 'interest', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', 'text', 'clustering', 'concept/entity', 'extraction', 'production', 'granular', 'taxonomies', 'sentiment', 'analysis', 'document', 'summarization', 'entity', 'relation', 'modeling', 'i.e.', 'learning', 'relations', 'named', 'entities']\n",
            "Stemmed Tokens: ['text', 'mine', 'text', 'data', 'mine', 'tdm', 'text', 'analyt', 'process', 'deriv', 'high-qual', 'inform', 'text', 'it', 'involv', '``', 'discoveri', 'comput', 'new', 'previous', 'unknown', 'inform', 'automat', 'extract', 'inform', 'differ', 'written', 'resourc', '``', '1', 'written', 'resourc', 'may', 'includ', 'websit', 'book', 'email', 'review', 'articl', 'high-qual', 'inform', 'typic', 'obtain', 'devis', 'pattern', 'trend', 'mean', 'statist', 'pattern', 'learn', 'accord', 'hotho', 'et', 'al', '2005', 'distinguish', 'three', 'differ', 'perspect', 'text', 'mine', 'inform', 'extract', 'data', 'mine', 'knowledg', 'discoveri', 'databas', 'kdd', 'process', '2', 'text', 'mine', 'usual', 'involv', 'process', 'structur', 'input', 'text', 'usual', 'pars', 'along', 'addit', 'deriv', 'linguist', 'featur', 'remov', 'other', 'subsequ', 'insert', 'databas', 'deriv', 'pattern', 'within', 'structur', 'data', 'final', 'evalu', 'interpret', 'output', \"'high\", 'qualiti', 'text', 'mine', 'usual', 'refer', 'combin', 'relev', 'novelti', 'interest', 'typic', 'text', 'mine', 'task', 'includ', 'text', 'categor', 'text', 'cluster', 'concept/ent', 'extract', 'product', 'granular', 'taxonomi', 'sentiment', 'analysi', 'document', 'summar', 'entiti', 'relat', 'model', 'i.e.', 'learn', 'relat', 'name', 'entiti']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)"
      ],
      "metadata": {
        "id": "JgrTyZ_eIReu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9071f357-ad16-4313-f8e0-0b11d71bf9ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"[1] Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\n",
            "\n",
            "Lemmatized Text: text mining , text data mining ( TDM ) or text analytic be the process of derive high - quality information from text . it involve \" the discovery by computer of new , previously unknown information , by automatically extract information from different write resource . \"[1 ] write resource may include website , book , email , review , and article . high - quality information be typically obtain by devise pattern and trend by mean such as statistical pattern learn . accord to Hotho et al . ( 2005 ) we can distinguish between three different perspective of text mining : information extraction , data mining , and a knowledge discovery in database ( KDD ) process.[2 ] text mining usually involve the process of structure the input text ( usually parse , along with the addition of some derive linguistic feature and the removal of other , and subsequent insertion into a database ) , derive pattern within the structured datum , and finally evaluation and interpretation of the output . ' high quality ' in text mining usually refer to some combination of relevance , novelty , and interest . typical text mining task include text categorization , text clustering , concept / entity extraction , production of granular taxonomy , sentiment analysis , document summarization , and entity relation modeling ( i.e. , learn relation between name entity ) . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "jgLGJFLQIUt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f78345-6458-4f88-838e-702f519bd4ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "tokens=word_tokenize(text)\n",
        "pos_=pos_tag(tokens)\n",
        "\n",
        "\n",
        "print(\"Tokens:\",tokens)\n",
        "print(\"PoS tags:\",pos_)"
      ],
      "metadata": {
        "id": "i4kgHUvXIWQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e12d07-bd44-4460-8296-6fd480a7ca17"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Text', 'mining', ',', 'text', 'data', 'mining', '(', 'TDM', ')', 'or', 'text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'high-quality', 'information', 'from', 'text', '.', 'It', 'involves', '``', 'the', 'discovery', 'by', 'computer', 'of', 'new', ',', 'previously', 'unknown', 'information', ',', 'by', 'automatically', 'extracting', 'information', 'from', 'different', 'written', 'resources', '.', '``', '[', '1', ']', 'Written', 'resources', 'may', 'include', 'websites', ',', 'books', ',', 'emails', ',', 'reviews', ',', 'and', 'articles', '.', 'High-quality', 'information', 'is', 'typically', 'obtained', 'by', 'devising', 'patterns', 'and', 'trends', 'by', 'means', 'such', 'as', 'statistical', 'pattern', 'learning', '.', 'According', 'to', 'Hotho', 'et', 'al', '.', '(', '2005', ')', 'we', 'can', 'distinguish', 'between', 'three', 'different', 'perspectives', 'of', 'text', 'mining', ':', 'information', 'extraction', ',', 'data', 'mining', ',', 'and', 'a', 'knowledge', 'discovery', 'in', 'databases', '(', 'KDD', ')', 'process', '.', '[', '2', ']', 'Text', 'mining', 'usually', 'involves', 'the', 'process', 'of', 'structuring', 'the', 'input', 'text', '(', 'usually', 'parsing', ',', 'along', 'with', 'the', 'addition', 'of', 'some', 'derived', 'linguistic', 'features', 'and', 'the', 'removal', 'of', 'others', ',', 'and', 'subsequent', 'insertion', 'into', 'a', 'database', ')', ',', 'deriving', 'patterns', 'within', 'the', 'structured', 'data', ',', 'and', 'finally', 'evaluation', 'and', 'interpretation', 'of', 'the', 'output', '.', \"'High\", 'quality', \"'\", 'in', 'text', 'mining', 'usually', 'refers', 'to', 'some', 'combination', 'of', 'relevance', ',', 'novelty', ',', 'and', 'interest', '.', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', ',', 'text', 'clustering', ',', 'concept/entity', 'extraction', ',', 'production', 'of', 'granular', 'taxonomies', ',', 'sentiment', 'analysis', ',', 'document', 'summarization', ',', 'and', 'entity', 'relation', 'modeling', '(', 'i.e.', ',', 'learning', 'relations', 'between', 'named', 'entities', ')', '.']\n",
            "PoS tags: [('Text', 'NNP'), ('mining', 'NN'), (',', ','), ('text', 'NN'), ('data', 'NNS'), ('mining', 'NN'), ('(', '('), ('TDM', 'NNP'), (')', ')'), ('or', 'CC'), ('text', 'JJ'), ('analytics', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('deriving', 'VBG'), ('high-quality', 'JJ'), ('information', 'NN'), ('from', 'IN'), ('text', 'NN'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('``', '``'), ('the', 'DT'), ('discovery', 'NN'), ('by', 'IN'), ('computer', 'NN'), ('of', 'IN'), ('new', 'JJ'), (',', ','), ('previously', 'RB'), ('unknown', 'JJ'), ('information', 'NN'), (',', ','), ('by', 'IN'), ('automatically', 'RB'), ('extracting', 'VBG'), ('information', 'NN'), ('from', 'IN'), ('different', 'JJ'), ('written', 'VBN'), ('resources', 'NNS'), ('.', '.'), ('``', '``'), ('[', 'JJ'), ('1', 'CD'), (']', 'NNS'), ('Written', 'NNP'), ('resources', 'NNS'), ('may', 'MD'), ('include', 'VB'), ('websites', 'NNS'), (',', ','), ('books', 'NNS'), (',', ','), ('emails', 'NNS'), (',', ','), ('reviews', 'NNS'), (',', ','), ('and', 'CC'), ('articles', 'NNS'), ('.', '.'), ('High-quality', 'NN'), ('information', 'NN'), ('is', 'VBZ'), ('typically', 'RB'), ('obtained', 'VBN'), ('by', 'IN'), ('devising', 'VBG'), ('patterns', 'NNS'), ('and', 'CC'), ('trends', 'NNS'), ('by', 'IN'), ('means', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('statistical', 'JJ'), ('pattern', 'NN'), ('learning', 'NN'), ('.', '.'), ('According', 'VBG'), ('to', 'TO'), ('Hotho', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.'), ('(', '('), ('2005', 'CD'), (')', ')'), ('we', 'PRP'), ('can', 'MD'), ('distinguish', 'VB'), ('between', 'IN'), ('three', 'CD'), ('different', 'JJ'), ('perspectives', 'NNS'), ('of', 'IN'), ('text', 'NN'), ('mining', 'NN'), (':', ':'), ('information', 'NN'), ('extraction', 'NN'), (',', ','), ('data', 'NN'), ('mining', 'NN'), (',', ','), ('and', 'CC'), ('a', 'DT'), ('knowledge', 'NN'), ('discovery', 'NN'), ('in', 'IN'), ('databases', 'NNS'), ('(', '('), ('KDD', 'NNP'), (')', ')'), ('process', 'NN'), ('.', '.'), ('[', 'CC'), ('2', 'CD'), (']', 'JJ'), ('Text', 'NNP'), ('mining', 'NN'), ('usually', 'RB'), ('involves', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('structuring', 'VBG'), ('the', 'DT'), ('input', 'NN'), ('text', 'NN'), ('(', '('), ('usually', 'RB'), ('parsing', 'VBG'), (',', ','), ('along', 'IN'), ('with', 'IN'), ('the', 'DT'), ('addition', 'NN'), ('of', 'IN'), ('some', 'DT'), ('derived', 'VBN'), ('linguistic', 'JJ'), ('features', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('removal', 'NN'), ('of', 'IN'), ('others', 'NNS'), (',', ','), ('and', 'CC'), ('subsequent', 'JJ'), ('insertion', 'NN'), ('into', 'IN'), ('a', 'DT'), ('database', 'NN'), (')', ')'), (',', ','), ('deriving', 'VBG'), ('patterns', 'NNS'), ('within', 'IN'), ('the', 'DT'), ('structured', 'JJ'), ('data', 'NNS'), (',', ','), ('and', 'CC'), ('finally', 'RB'), ('evaluation', 'NN'), ('and', 'CC'), ('interpretation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('output', 'NN'), ('.', '.'), (\"'High\", 'JJ'), ('quality', 'NN'), (\"'\", \"''\"), ('in', 'IN'), ('text', 'JJ'), ('mining', 'NN'), ('usually', 'RB'), ('refers', 'VBZ'), ('to', 'TO'), ('some', 'DT'), ('combination', 'NN'), ('of', 'IN'), ('relevance', 'NN'), (',', ','), ('novelty', 'NN'), (',', ','), ('and', 'CC'), ('interest', 'NN'), ('.', '.'), ('Typical', 'JJ'), ('text', 'NN'), ('mining', 'NN'), ('tasks', 'NNS'), ('include', 'VBP'), ('text', 'JJ'), ('categorization', 'NN'), (',', ','), ('text', 'NN'), ('clustering', 'NN'), (',', ','), ('concept/entity', 'NN'), ('extraction', 'NN'), (',', ','), ('production', 'NN'), ('of', 'IN'), ('granular', 'JJ'), ('taxonomies', 'NNS'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('document', 'NN'), ('summarization', 'NN'), (',', ','), ('and', 'CC'), ('entity', 'NN'), ('relation', 'NN'), ('modeling', 'NN'), ('(', '('), ('i.e.', 'FW'), (',', ','), ('learning', 'VBG'), ('relations', 'NNS'), ('between', 'IN'), ('named', 'VBN'), ('entities', 'NNS'), (')', ')'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "YFiAef3wIdkD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3442fa8d-df6d-4159-b8ff-2e61d0e82fad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "id": "-oCiLbSHId_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bdaf5ce-db91-4c4b-ff10-71af03c36c38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "for chunk in ne_chunk(nltk.pos_tag(word_tokenize(text))):\n",
        "        if hasattr(chunk, 'label'):\n",
        "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "metadata": {
        "id": "Yaj8XVkfIjKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0b2ace-e53f-4d13-accc-30b34807419e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE Text\n",
            "ORGANIZATION TDM\n",
            "GPE Hotho\n",
            "ORGANIZATION KDD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")\n",
        "install.packages(\"tm\")\n",
        "install.packages(\"textstem\")"
      ],
      "metadata": {
        "id": "_SGg6VcKIjq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba04abd-4157-402f-ad9f-cf10c5a7b752"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text analytics in R\n",
        "\n",
        "library(tokenizers)\n",
        "\n",
        "text <- readline(prompt = \"Enter text: \")\n",
        "\n",
        "word_tokens <- unlist(tokenize_words(text))\n",
        "sentence_tokens <- unlist(tokenize_sentences(text))\n",
        "\n",
        "cat(\"\\nTokenized words:\\n\")\n",
        "print(word_tokens)\n",
        "\n",
        "cat(\"\\nTokenized sentences:\\n\")\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "id": "9YuSeV_QImXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e648afc9-dc18-4801-afa1-383d4f8c7891"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: Tokenization is the first step in text analytics. The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization. The token is a single entity that is building blocks for sentences or paragraphs.\n",
            "\n",
            "Tokenized words:\n",
            " [1] \"tokenization\" \"is\"           \"the\"          \"first\"        \"step\"        \n",
            " [6] \"in\"           \"text\"         \"analytics\"    \"the\"          \"process\"     \n",
            "[11] \"of\"           \"breaking\"     \"down\"         \"text\"         \"paragraphs\"  \n",
            "[16] \"into\"         \"smaller\"      \"chunks\"       \"such\"         \"as\"          \n",
            "[21] \"words\"        \"or\"           \"sentences\"    \"is\"           \"called\"      \n",
            "[26] \"tokenization\" \"the\"          \"token\"        \"is\"           \"a\"           \n",
            "[31] \"single\"       \"entity\"       \"that\"         \"is\"           \"building\"    \n",
            "[36] \"blocks\"       \"for\"          \"sentences\"    \"or\"           \"paragraphs\"  \n",
            "\n",
            "Tokenized sentences:\n",
            "[1] \"Tokenization is the first step in text analytics.\"                                                                  \n",
            "[2] \"The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization.\"\n",
            "[3] \"The token is a single entity that is building blocks for sentences or paragraphs.\"                                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "word_freq <- table(word_tokens)\n",
        "\n",
        "print(\"Most common words:\")\n",
        "print(head(sort(word_freq, decreasing = TRUE), 2))\n",
        "\n",
        "print(\"Frequency of each word:\")\n",
        "print(word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcadWU4RJqWO",
        "outputId": "d9309a65-9fae-4c48-b375-eeb12f410c84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Most common words:\"\n",
            "word_tokens\n",
            " is the \n",
            "  4   3 \n",
            "[1] \"Frequency of each word:\"\n",
            "word_tokens\n",
            "           a    analytics           as       blocks     breaking     building \n",
            "           1            1            1            1            1            1 \n",
            "      called       chunks         down       entity        first          for \n",
            "           1            1            1            1            1            1 \n",
            "          in         into           is           of           or   paragraphs \n",
            "           1            1            4            1            2            2 \n",
            "     process    sentences       single      smaller         step         such \n",
            "           1            2            1            1            1            1 \n",
            "        text         that          the        token tokenization        words \n",
            "           2            1            3            1            2            1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(tm)\n",
        "\n",
        "filtered_tokens <- word_tokens[!word_tokens %in% stopwords(\"en\")]\n",
        "\n",
        "print(\"Filtered Tokens:\")\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARql6UEHJrZs",
        "outputId": "9425dccf-fc4d-4add-b247-ecbf0b8ce27b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: NLP\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Filtered Tokens:\"\n",
            " [1] \"tokenization\" \"first\"        \"step\"         \"text\"         \"analytics\"   \n",
            " [6] \"process\"      \"breaking\"     \"text\"         \"paragraphs\"   \"smaller\"     \n",
            "[11] \"chunks\"       \"words\"        \"sentences\"    \"called\"       \"tokenization\"\n",
            "[16] \"token\"        \"single\"       \"entity\"       \"building\"     \"blocks\"      \n",
            "[21] \"sentences\"    \"paragraphs\"  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming <- function(text) {\n",
        "  corpus <- Corpus(VectorSource(text))\n",
        "  corpus <- tm_map(corpus, stemDocument)\n",
        "  return(corpus)\n",
        "}\n",
        "\n",
        "stemmed_corpus <- stemming(filtered_tokens)\n",
        "\n",
        "print(\"Stemmed Tokens:\")\n",
        "print(unlist(sapply(stemmed_corpus, as.character)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwY2gMn6Jt13",
        "outputId": "d747d4cc-4b04-48d1-f53b-be4c253251dc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in tm_map.SimpleCorpus(corpus, stemDocument):\n",
            "“transformation drops documents”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Stemmed Tokens:\"\n",
            " [1] \"token\"     \"first\"     \"step\"      \"text\"      \"analyt\"    \"process\"  \n",
            " [7] \"break\"     \"text\"      \"paragraph\" \"smaller\"   \"chunk\"     \"word\"     \n",
            "[13] \"sentenc\"   \"call\"      \"token\"     \"token\"     \"singl\"     \"entiti\"   \n",
            "[19] \"build\"     \"block\"     \"sentenc\"   \"paragraph\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and load the textstem package\n",
        "install.packages(\"textstem\")\n",
        "library(textstem)\n",
        "\n",
        "# Define the lemmatization function\n",
        "lemmatization <- function(text) {\n",
        "  # Tokenize the text\n",
        "  tokens <- unlist(strsplit(text, \"\\\\s+\"))\n",
        "\n",
        "  # Lemmatize the tokens\n",
        "  lemmatized_tokens <- lemmatize_words(tokens)\n",
        "\n",
        "  # Combine lemmatized tokens into a string\n",
        "  lemmatized_text <- paste(lemmatized_tokens, collapse = \" \")\n",
        "\n",
        "  return(lemmatized_text)\n",
        "}\n",
        "\n",
        "text <- \"I am running towards the horses\"\n",
        "lemmatized_text <- lemmatization(text)\n",
        "\n",
        "print(lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM3d4jvYJwHK",
        "outputId": "f0965445-2938-4716-98c8-6fcaad0a8883"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Loading required package: koRpus.lang.en\n",
            "\n",
            "Loading required package: koRpus\n",
            "\n",
            "Loading required package: sylly\n",
            "\n",
            "For information on available language packages for 'koRpus', run\n",
            "\n",
            "  available.koRpus.lang()\n",
            "\n",
            "and see ?install.koRpus.lang()\n",
            "\n",
            "\n",
            "\n",
            "Attaching package: ‘koRpus’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:tm’:\n",
            "\n",
            "    readTagged\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"I be run towards the horse\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(rvest)\n",
        "\n",
        "url <- 'https://en.wikipedia.org/wiki/Text_mining'\n",
        "\n",
        "page <- read_html(url)\n",
        "\n",
        "if (!is.null(page)) {\n",
        "  print(page)\n",
        "\n",
        "  first_paragraph <- html_nodes(page, 'p')[1]\n",
        "\n",
        "  if (!is.null(first_paragraph)) {\n",
        "    print(html_text(first_paragraph))\n",
        "  } else {\n",
        "    print('No <p> tags found on the website')\n",
        "  }\n",
        "} else {\n",
        "  print('Failed to retrieve data from the website')\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SOVg2RzJywe",
        "outputId": "53952f12-9cb1-4403-83e7-f2e9484011a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{html_document}\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-feature-night-mode-disabled skin-night-mode-clientpref-0 vector-toc-available\" lang=\"en\" dir=\"ltr\">\n",
            "[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n",
            "[2] <body class=\"skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr ...\n",
            "[1] \"Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \\\"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\\\"[1] Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B3DdAfJ_J93B"
      }
    }
  ]
}